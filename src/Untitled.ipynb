{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7737dd2-8bc3-4cd8-9907-2e5f7cb0d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Vector Store, Embedded Instance\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "import hnswlib\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "class Store:\n",
    "    def __init__(self, name, path=None, persistent=False, max_elements=100000):\n",
    "        self.name = name\n",
    "        if persistent:\n",
    "            assert path is not None\n",
    "            self.con = sqlite3.connect(os.path.join(path, \"temp.db\"))\n",
    "        else:\n",
    "            self.con = sqlite3.connect(\":memory:\")\n",
    "            \n",
    "            \n",
    "        self.cur = self.con.cursor()\n",
    "        self.cur.execute(\"CREATE TABLE {}(id INT NOT NULL PRIMARY KEY, img_url TEXT NOT NULL)\".format(self.name + \"_imagestore\"))\n",
    "        self.cur.execute(\"CREATE TABLE {}(id INT NOT NULL PRIMARY KEY, text TEXT NOT NULL)\".format(self.name + \"_textstore\"))\n",
    "        self.max_elements = max_elements\n",
    "        self.dim = None\n",
    "        self.idx = None\n",
    "        \n",
    "    def insert(self, ids, embeddings, items, datatype):\n",
    "        \n",
    "        assert datatype in [\"image\", \"text\"]\n",
    "        \n",
    "        # if first insertion, set dim and construct index\n",
    "        if self.dim == None:\n",
    "            self.dim = embeddings.squeeze().shape[-1]\n",
    "        if self.idx == None:\n",
    "            self.idx = hnswlib.Index(space='l2', dim=self.dim)\n",
    "            self.idx.init_index(max_elements=self.max_elements, ef_construction=20, M=100)\n",
    "            self.idx.set_ef(60)\n",
    "        \n",
    "        # add embeddings to index\n",
    "        self.idx.add_items(embeddings, ids)\n",
    "        \n",
    "        # add to sqlite store\n",
    "        data = list(zip(ids, items))\n",
    "        if datatype == \"image\":\n",
    "            self.cur.executemany(\"INSERT INTO {} VALUES (?, ?)\".format(self.name + \"_imagestore\"), data)\n",
    "        elif datatype == \"text\":\n",
    "            self.cur.executemany(\"INSERT INTO {} VALUES (?, ?)\".format(self.name + \"_textstore\"), data)\n",
    "            \n",
    "        self.con.commit()\n",
    "    \n",
    "    def delete(self, ids):\n",
    "        self.cur.executemany(\"DELETE FROM {} WHERE ids = ?\".format(self.name + \"_imagestore\"), ids)\n",
    "        self.con.commit()\n",
    "        self.cur.executemany(\"DELETE FROM {} WHERE ids = ?\".format(self.name + \"_textstore\"), ids)\n",
    "        self.con.commit()\n",
    "    \n",
    "    def query(self, queries, n_results_per_query):\n",
    "        \n",
    "        # get ids from hnsw index\n",
    "        ids, distances = self.idx.knn_query(queries, k=n_results_per_query)\n",
    "        ids = ids.tolist()\n",
    "        \n",
    "        #get objects from stores\n",
    "        out = {\"ids\": [], \"images\": [], \"text\": []}\n",
    "        for i in range(len(queries)):\n",
    "            res_images = self.cur.execute('SELECT img_url FROM {} WHERE id IN ({})'.format(self.name + \"_imagestore\", ', '.join('?' for _ in ids[i])), ids[i]).fetchall()\n",
    "            res_text = self.cur.execute('SELECT text FROM {} WHERE id IN ({})'.format(self.name + \"_textstore\", ', '.join('?' for _ in ids[i])), ids[i]).fetchall()\n",
    "            self.con.commit()\n",
    "            out['ids'].append(ids[i])\n",
    "            out['images'].append(res_images)\n",
    "            out['text'].append(res_text)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_pil_images(self, query_result):\n",
    "        impaths = query_result['images']\n",
    "        out = []\n",
    "        for query_idx in range(len(impaths)):\n",
    "            current_out = []\n",
    "            current_query = impaths[query_idx]\n",
    "            for img_idx in range(len(current_query)):\n",
    "                current_impath = current_query[img_idx][0]\n",
    "                current_out.append(Image.open(current_impath))\n",
    "            out.append(current_out)\n",
    "        return out\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aafa829b-135f-4cfd-9da9-489239bb94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedder Providers\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, CLIPTextModelWithProjection, AutoProcessor, CLIPVisionModelWithProjection\n",
    "from PIL import Image\n",
    "\n",
    "class CLIPEmbedder:\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", processor_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.model_name = model_name\n",
    "        self.processor_name = processor_name\n",
    "        \n",
    "    def __call__(self, text=None, images=None):\n",
    "        \n",
    "        if images is not None:\n",
    "            for i in range(len(images)):\n",
    "                if type(images[i]) == str:\n",
    "                    images[i] = Image.open(images[i])\n",
    "        \n",
    "        if images is None and text is None:\n",
    "            return\n",
    "        \n",
    "        if images is not None and text is None:\n",
    "            model = CLIPVisionModelWithProjection.from_pretrained(self.model_name)\n",
    "            processor = AutoProcessor.from_pretrained(self.processor_name)\n",
    "            inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            return outputs.image_embeds\n",
    "        \n",
    "        if images is None and text is not None:\n",
    "            model = CLIPTextModelWithProjection.from_pretrained(self.model_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.processor_name)\n",
    "            inputs = tokenizer(text=text, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            return outputs.text_embeds\n",
    "        \n",
    "        if images is not None and text is not None:\n",
    "            model = CLIPModel.from_pretrained(self.model_name)\n",
    "            processor = CLIPProcessor.from_pretrained(self.processor_name)\n",
    "            inputs = processor(text=text, images=images, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            return (outputs.text_embeds, outputs.image_embeds)\n",
    "    \n",
    "    def embed(self, text=None, images=None):\n",
    "        return self.__call__(text, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb0305-1fea-4bfa-a85b-331f1de0ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hosted Server\n",
    "\n",
    "from starlette.applications import Starlette\n",
    "from starlette.responses import JSONResponse\n",
    "from starlette.routing import Route\n",
    "import uvicorn\n",
    "\n",
    "class Server:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def start(self, store):\n",
    "\n",
    "        async def insert(request):\n",
    "            #store.insert()\n",
    "            return JSONResponse({'hello': 'world' + request.path_params[]})\n",
    "        \n",
    "        async def delete(request):\n",
    "            #store.delete()\n",
    "            return JSONResponse({'hello': 'world'})\n",
    "        \n",
    "        async def query(request):\n",
    "            #store.query()\n",
    "            return JSONResponse({'hello': 'world'})\n",
    "\n",
    "        app = Starlette(debug=True, routes=[\n",
    "            Route('/insert', insert),\n",
    "            Route('/delete', delete),\n",
    "            Route('/query', query),\n",
    "        ])\n",
    "        \n",
    "        uvicorn.run(app)\n",
    "\n",
    "    \n",
    "    def terminate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c122a-a65a-4c67-b8b4-629c5d6c9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client\n",
    "\n",
    "class Client:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d132c-f1fc-4b59-b68f-3e1b7b27b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Instance\n",
    "\n",
    "class Instance:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd9c07-76ad-4162-af0d-b1520adca72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0c8b2246-0253-4159-aac3-b119f963347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "ids = list(range(1000))\n",
    "c = CLIPEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73328ee3-a6df-4ec6-a604-4bd8af3a9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Store(name=\"ungus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b294515-386d-4d66-bcb6-d00ae6df06c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 (1, 512) images/3396004.jpg\n",
      "11 (1, 512) images/243004.jpg\n",
      "12 (1, 512) images/3121004.jpg\n"
     ]
    }
   ],
   "source": [
    "images = glob.glob(\"images/*\")\n",
    "for i in range(10, 13):\n",
    "    im = Image.open(images[i])\n",
    "    emb = c.embed(images=[im]).detach().numpy()\n",
    "    print(i, emb.shape, images[i])\n",
    "    store.insert([i], emb, [images[i]], datatype=\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d64754b1-e0a4-48c0-824f-e9dd225be987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM ungus_imagestore WHERE id IN (?, ?)\n",
      "SELECT * FROM ungus_imagestore WHERE id IN (?, ?)\n"
     ]
    }
   ],
   "source": [
    "q = [np.random.rand(512), np.random.rand(512)]\n",
    "a = store.get_pil_images(store.query(q, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234c85a-40a7-4a3a-919a-bbf3972f616e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd562f60-5902-4873-9623-54d4f5388d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893c83a-a833-4efb-a9b7-5d86a1c98c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force approx nearest neighbor\n",
    "\n",
    "import gradio as gr\n",
    "import sqlite3\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor, CLIPVisionModelWithProjection, CLIPTextModelWithProjection\n",
    "\n",
    "def randomword(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "tablename = randomword(16)\n",
    "\n",
    "def build_vector_db(image_dir, progress=gr.Progress()):\n",
    "\n",
    "    global tablename\n",
    "    tablename = randomword(16)\n",
    "    con = sqlite3.connect(\"temp.db\")\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"CREATE TABLE {}(emb text primary key, image text)\".format(tablename))\n",
    "\n",
    "    model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    for d in progress.tqdm(image_dir, desc=\"Building\"):\n",
    "\n",
    "        if (\"jpg\" not in d.name) and (\"png\" not in d.name):\n",
    "            continue\n",
    "        image = Image.open(d.name)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        image_embed = outputs.image_embeds.squeeze().tolist()\n",
    "        image_embed_json = json.dumps(image_embed)\n",
    "        cur.execute(\"INSERT INTO {} VALUES(\\\"{}\\\", \\\"{}\\\")\".format(tablename, image_embed_json, d.name))\n",
    "        con.commit()\n",
    "\n",
    "    con.close()\n",
    "    return \"Vector DB Ready\"\n",
    "\n",
    "def search_query(query):\n",
    "\n",
    "    con = sqlite3.connect(\"temp.db\")\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = tokenizer([query], return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    text_embed = outputs.text_embeds.squeeze()\n",
    "\n",
    "    min_image_path = None\n",
    "    min_dist = math.inf\n",
    "\n",
    "    for row in cur.execute(\"SELECT * FROM {}\".format(tablename)):\n",
    "        embedding_json = row[0]\n",
    "        image_embed = torch.Tensor(json.loads(embedding_json))\n",
    "        dist = torch.dist(image_embed, text_embed)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_image_path = row[1]\n",
    "\n",
    "    image = Image.open(min_image_path)\n",
    "    con.close()\n",
    "    return image\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    image_dir = gr.File(file_count=\"directory\",label=\"Input Files\", height=200)\n",
    "    upload = gr.Button(value=\"Build Vector DB\")\n",
    "    outtext = gr.Textbox()\n",
    "    upload.click(fn=build_vector_db, inputs=image_dir, outputs=outtext)\n",
    "    query = gr.Textbox(placeholder=\"Text Query Here\")\n",
    "    search = gr.Button(value=\"Search\")\n",
    "    image = gr.Image()\n",
    "    search.click(fn=search_query, inputs=query, outputs=image)\n",
    "\n",
    "interface.queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f12a9-2afe-4406-ad95-ef0f65ef9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# miscninja\n",
    "\n",
    "import gradio as gr\n",
    "import sqlite3\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoProcessor, CLIPVisionModelWithProjection, CLIPTextModelWithProjection\n",
    "\n",
    "def randomword(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "tablename = randomword(16)\n",
    "store = None\n",
    "\n",
    "def build_vector_db(image_dir, progress=gr.Progress()):\n",
    "\n",
    "    global tablename\n",
    "    global store\n",
    "    tablename = randomword(16)\n",
    "    store = Store(name=tablename)\n",
    "    c = CLIPEmbedder()\n",
    "\n",
    "    i = 0\n",
    "    for d in progress.tqdm(image_dir, desc=\"Building\"):\n",
    "        if (\"jpg\" not in d.name) and (\"png\" not in d.name):\n",
    "            continue\n",
    "        image = Image.open(d.name)\n",
    "        emb = c.embed(images=[image]).detach().numpy()\n",
    "        store.insert([i], emb, [d.name], datatype=\"image\")\n",
    "        i += 1\n",
    "        \n",
    "    return \"Vector DB Ready\"\n",
    "\n",
    "def search_query(query):\n",
    "    \n",
    "    c = CLIPEmbedder()\n",
    "    text_embed = c.embed(text=[query]).detach().squeeze()\n",
    "    image = random.choice(store.get_pil_images(store.query([text_embed.numpy()], 3))[0])\n",
    "    return image\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    image_dir = gr.File(file_count=\"directory\",label=\"Input Files\", height=200)\n",
    "    upload = gr.Button(value=\"Build Vector DB\")\n",
    "    outtext = gr.Textbox()\n",
    "    upload.click(fn=build_vector_db, inputs=image_dir, outputs=outtext)\n",
    "    query = gr.Textbox(placeholder=\"Text Query Here\")\n",
    "    search = gr.Button(value=\"Search\")\n",
    "    image = gr.Image()\n",
    "    search.click(fn=search_query, inputs=query, outputs=image)\n",
    "\n",
    "interface.queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b27e92-c151-429a-ae79-47577c887bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38822e-0d8f-4a96-a5c3-f09afe11da34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "799eb4b2-fbfc-47e2-adcb-c0319391d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stores.multimodal_sqlite_hnsw import MultimodalSQLiteHNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e627483-ae15-4c23-8e4e-579850dd32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MultimodalSQLiteHNSW(\"asdf\", 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "051fab8f-0c2d-4f7a-a0e5-e93f9500da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b64482a-c251-43cf-8cfd-6a0bc0459f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(512)\n",
    "b = np.ones(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c843bb6e-d683-4cc0-a5a7-9aaefbbe9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.insert([1], [a], ['text'], 'text')\n",
    "m.insert([2], [b], ['image'], 'image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6feaf43f-d446-471c-b4b4-15d654a542be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ids': [[2]], 'images': [[('image',)]]}, {'ids': [[2]], 'text': [[]]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.query([b], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc1854-87c2-41c2-96d9-13dc199876c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
